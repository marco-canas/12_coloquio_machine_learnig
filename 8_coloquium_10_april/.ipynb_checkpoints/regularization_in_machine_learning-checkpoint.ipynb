{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cc0053",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/12_coloquio_machine_learnig/blob/main/8_coloquium_10_april/regularization_in_machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e2c746",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Video de apoyo a la lectura de este cuaderno jupyter]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4cc98e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Regularization en machine learning](https://medium.com/@roiyeho/regularization-19b1879415a1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da9c99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Regularization is a widespread technique that is used in many machine learning models to control the complexity of the model and thereby improve its generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e38519",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this article I’m going to explain the concept of regularization in depth, and demonstrate its usage in several machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31555d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c32e79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Regularization is a technique to prevent overfitting by penalizing complex models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a9acb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The idea is to add a penalty term to the cost function of the model, such that it becomes dependent on two factors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ae2a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ Cost(h) = \\text{training_error}(h) + \\lambda \\ \\text{Complexity}(h) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905038c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\lambda$ is a hyperparameter (called the regularization coefficient) that controls the tradeoff between the bias and the variance (for a discussion on the bias-variance tradeoff see my previous article). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fefd10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Higher $\\lambda$ will induce a larger penalty on the complexity of the model, and thus will lead to simpler models with higher error on the training set but with smaller variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bac073",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The complexity of the model can be measured in a variety of ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a85f42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, in models that consist of a vector of parameters (weights) $w$, such as linear regression or neural networks, we use the size of the parameters (the norm of the vector $w$) as a measure for the model’s complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f3b0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In such models, there are two common types of regularization, depending on the norm of the vector w that we are using:\n",
    "\n",
    "L1 regularization. In this case, we use the L1 norm of the vector w, i.e., the sum of the absolute values of the weights.\n",
    "For example, in linear regression, if we have m features in our data set, then the model will have m parameters (weights) plus a bias term, thus we can write the L1 norm of w as:\n",
    "\n",
    "$$ \\Vert w \\Vert_{1} = $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ee66a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd48ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9232eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ee37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "rise": {
   "enable_chalkboard": true,
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
